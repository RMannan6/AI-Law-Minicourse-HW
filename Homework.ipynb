
In [1]:

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import math
import os
import random
from tempfile import gettempdir
import zipfile

import numpy as np
from six.moves import urllib
from six.moves import xrange  # pylint: disable=redefined-builtin
import tensorflow as tf

# Step 1: Download the data.
url = 'http://mattmahoney.net/dc/'


# pylint: disable=redefined-outer-name
def maybe_download(filename, expected_bytes):
  """Download a file if not present, and make sure it's the right size."""
  local_filename = os.path.join(gettempdir(), filename)
  if not os.path.exists(local_filename):
    local_filename, _ = urllib.request.urlretrieve(url + filename,
                                                   local_filename)
  statinfo = os.stat(local_filename)
  if statinfo.st_size == expected_bytes:
    print('Found and verified', filename)
  else:
    print(statinfo.st_size)
    raise Exception('Failed to verify ' + local_filename +
                    '. Can you get to it with a browser?')
  return local_filename


filename = maybe_download('text8.zip', 31344016)


# Read the data into a list of strings.
def read_data(filename):
  """Extract the first file enclosed in a zip file as a list of words."""
  with zipfile.ZipFile(filename) as f:
    data = tf.compat.as_str(f.read(f.namelist()[0])).split()
  return data

vocabulary = read_data(filename)
print('Data size', len(vocabulary))

# Step 2: Build the dictionary and replace rare words with UNK token.
vocabulary_size = 50000


def build_dataset(words, n_words):
  """Process raw inputs into a dataset."""
  count = [['UNK', -1]]
  count.extend(collections.Counter(words).most_common(n_words - 1))
  dictionary = dict()
  for word, _ in count:
    dictionary[word] = len(dictionary)
  data = list()
  unk_count = 0
  for word in words:
    index = dictionary.get(word, 0)
    if index == 0:  # dictionary['UNK']
      unk_count += 1
    data.append(index)
  count[0][1] = unk_count
  reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))
  return data, count, dictionary, reversed_dictionary

# Filling 4 global variables:
# data - list of codes (integers from 0 to vocabulary_size-1).
#   This is the original text but words are replaced by their codes
# count - map of words(strings) to count of occurrences
# dictionary - map of words(strings) to their codes(integers)
# reverse_dictionary - maps codes(integers) to words(strings)
data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,
                                                            vocabulary_size)
del vocabulary  # Hint to reduce memory.
print('Most common words (+UNK)', count[:5])
print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])

data_index = 0

# Step 3: Function to generate a training batch for the skip-gram model.
def generate_batch(batch_size, num_skips, skip_window):
  global data_index
  assert batch_size % num_skips == 0
  assert num_skips <= 2 * skip_window
  batch = np.ndarray(shape=(batch_size), dtype=np.int32)
  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
  span = 2 * skip_window + 1  # [ skip_window target skip_window ]
  buffer = collections.deque(maxlen=span)
  if data_index + span > len(data):
    data_index = 0
  buffer.extend(data[data_index:data_index + span])
  data_index += span
  for i in range(batch_size // num_skips):
    context_words = [w for w in range(span) if w != skip_window]
    words_to_use = random.sample(context_words, num_skips)
    for j, context_word in enumerate(words_to_use):
      batch[i * num_skips + j] = buffer[skip_window]
      labels[i * num_skips + j, 0] = buffer[context_word]
    if data_index == len(data):
      buffer[:] = data[:span]
      data_index = span
    else:
      buffer.append(data[data_index])
      data_index += 1
  # Backtrack a little bit to avoid skipping words in the end of a batch
  data_index = (data_index + len(data) - span) % len(data)
  return batch, labels

batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)
for i in range(8):
  print(batch[i], reverse_dictionary[batch[i]],
        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])

# Step 4: Build and train a skip-gram model.

batch_size = 128
embedding_size = 128  # Dimension of the embedding vector.
skip_window = 1       # How many words to consider left and right.
num_skips = 2         # How many times to reuse an input to generate a label.
num_sampled = 64      # Number of negative examples to sample.

# We pick a random validation set to sample nearest neighbors. Here we limit the
# validation samples to the words that have a low numeric ID, which by
# construction are also the most frequent. These 3 variables are used only for
# displaying model accuracy, they don't affect calculation.
valid_size = 16     # Random set of words to evaluate similarity on.
valid_window = 100  # Only pick dev samples in the head of the distribution.
valid_examples = np.random.choice(valid_window, valid_size, replace=False)


graph = tf.Graph()

with graph.as_default():

  # Input data.
  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

  # Ops and variables pinned to the CPU because of missing GPU implementation
  with tf.device('/cpu:0'):
    # Look up embeddings for inputs.
    embeddings = tf.Variable(
        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    embed = tf.nn.embedding_lookup(embeddings, train_inputs)

    # Construct the variables for the NCE loss
    nce_weights = tf.Variable(
        tf.truncated_normal([vocabulary_size, embedding_size],
                            stddev=1.0 / math.sqrt(embedding_size)))
    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))

  # Compute the average NCE loss for the batch.
  # tf.nce_loss automatically draws a new sample of the negative labels each
  # time we evaluate the loss.
  # Explanation of the meaning of NCE loss:
  #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/
  loss = tf.reduce_mean(
      tf.nn.nce_loss(weights=nce_weights,
                     biases=nce_biases,
                     labels=train_labels,
                     inputs=embed,
                     num_sampled=num_sampled,
                     num_classes=vocabulary_size))

  # Construct the SGD optimizer using a learning rate of 1.0.
  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)

  # Compute the cosine similarity between minibatch examples and all embeddings.
  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
  normalized_embeddings = embeddings / norm
  valid_embeddings = tf.nn.embedding_lookup(
      normalized_embeddings, valid_dataset)
  similarity = tf.matmul(
      valid_embeddings, normalized_embeddings, transpose_b=True)

  # Add variable initializer.
  init = tf.global_variables_initializer()

# Step 5: Begin training.
num_steps = 100001

with tf.Session(graph=graph) as session:
  # We must initialize all variables before we use them.
  init.run()
  print('Initialized')

  average_loss = 0
  for step in xrange(num_steps):
    batch_inputs, batch_labels = generate_batch(
        batch_size, num_skips, skip_window)
    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}

    # We perform one update step by evaluating the optimizer op (including it
    # in the list of returned values for session.run()
    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)
    average_loss += loss_val

    if step % 2000 == 0:
      if step > 0:
        average_loss /= 2000
      # The average loss is an estimate of the loss over the last 2000 batches.
      print('Average loss at step ', step, ': ', average_loss)
      average_loss = 0

    # Note that this is expensive (~20% slowdown if computed every 500 steps)
    if step % 10000 == 0:
      sim = similarity.eval()
      for i in xrange(valid_size):
        valid_word = reverse_dictionary[valid_examples[i]]
        top_k = 8  # number of nearest neighbors
        nearest = (-sim[i, :]).argsort()[1:top_k + 1]
        log_str = 'Nearest to %s:' % valid_word
        for k in xrange(top_k):
          close_word = reverse_dictionary[nearest[k]]
          log_str = '%s %s,' % (log_str, close_word)
        print(log_str)
  final_embeddings = normalized_embeddings.eval()

# Step 6: Visualize the embeddings.


# pylint: disable=missing-docstring
# Function to draw visualization of distance between embeddings.
def plot_with_labels(low_dim_embs, labels, filename):
  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'
  plt.figure(figsize=(18, 18))  # in inches
  for i, label in enumerate(labels):
    x, y = low_dim_embs[i, :]
    plt.scatter(x, y)
    plt.annotate(label,
                 xy=(x, y),
                 xytext=(5, 2),
                 textcoords='offset points',
                 ha='right',
                 va='bottom')

  plt.savefig(filename)

try:
  # pylint: disable=g-import-not-at-top
  from sklearn.manifold import TSNE
  import matplotlib.pyplot as plt

  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')
  plot_only = 500
  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])
  labels = [reverse_dictionary[i] for i in xrange(plot_only)]
  plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(), 'tsne.png'))

except ImportError as ex:
  print('Please install sklearn, matplotlib, and scipy to show embeddings.')
  print(ex)

C:\Users\sannaluoma\Anaconda3\lib\site-packages\h5py\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

Found and verified text8.zip
Data size 17005207
Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]
Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']
3081 originated -> 12 as
3081 originated -> 5234 anarchism
12 as -> 3081 originated
12 as -> 6 a
6 a -> 195 term
6 a -> 12 as
195 term -> 6 a
195 term -> 2 of
Initialized
Average loss at step  0 :  326.134521484375
Nearest to have: pomeranian, mixer, quicker, planners, exploring, inscription, unspecific, schottenheimer,
Nearest to UNK: counterbalanced, stuka, helga, revive, primal, replicating, upstream, hauge,
Nearest to also: luisa, responsible, efforts, samadhi, domingo, novels, stratigraphic, ebbinghaus,
Nearest to while: industrious, exhibit, valuation, hag, clocked, indulgences, circus, thaler,
Nearest to only: technical, frankfurter, repeating, retired, classifications, clockmaker, samuelson, lomo,
Nearest to had: aachen, gangster, testing, justus, folds, generalizations, cygwin, marie,
Nearest to eight: commanding, hbar, suicides, ras, ethologists, ennedi, tse, subway,
Nearest to its: simultaneously, pounder, slugger, jupiter, radar, neck, hillbillies, solemn,
Nearest to would: etsi, thread, favors, ende, profitable, satisfactory, category, ness,
Nearest to nine: denies, dormant, unimpressed, forbid, receiving, check, faisal, levant,
Nearest to b: played, sluggish, morbidity, optimates, cimmeria, cache, massif, rio,
Nearest to american: marcion, judge, chesapeake, wesleyan, gestation, tolstoy, declarations, nims,
Nearest to all: imparts, prompts, fail, found, stave, vulnerable, krona, withered,
Nearest to many: fabian, asking, ungodly, bale, readers, abandonment, firstnode, pdas,
Nearest to during: inherently, convening, honecker, customized, jest, severed, keillor, evacuate,
Nearest to from: girls, virgil, doubleday, agents, engagements, affixes, shamanistic, sayers,
Average loss at step  2000 :  113.32230447769165
Average loss at step  4000 :  53.126729866981506
Average loss at step  6000 :  33.46294644665718
Average loss at step  8000 :  23.835717922925948
Average loss at step  10000 :  17.732971609711647
Nearest to have: mixer, eighth, algerian, are, were, amazon, fixed, jet,
Nearest to UNK: vocals, vs, and, one, phi, tissue, the, hakama,
Nearest to also: responsible, novels, efforts, path, renamed, as, coercion, interest,
Nearest to while: december, exhibit, affected, member, blended, valuation, flow, ad,
Nearest to only: technical, retired, ep, classifications, bckgr, wind, mathbf, host,
Nearest to had: efforts, pregnancies, testing, fiction, lee, contribute, counted, gestation,
Nearest to eight: nine, zero, five, three, gland, four, tissue, mosque,
Nearest to its: simultaneously, jupiter, former, the, means, mission, vs, ep,
Nearest to would: favors, rsha, thread, profitable, category, factors, compromise, howard,
Nearest to nine: zero, eight, seven, six, one, coke, five, two,
Nearest to b: extends, played, lnot, receiving, elephant, farther, subsets, delivered,
Nearest to american: judge, gestation, tolstoy, chesapeake, and, jane, elision, declarations,
Nearest to all: found, defining, mosque, wheat, embryo, fail, vulnerable, pogroms,
Nearest to many: the, fabian, ungodly, lager, proceeding, vs, readers, abandonment,
Nearest to during: transferred, inherently, mathbf, observe, supporter, retail, severed, evacuate,
Nearest to from: in, and, of, at, sayers, altenberg, var, for,
Average loss at step  12000 :  13.989586282372475
Average loss at step  14000 :  11.595134418725968
Average loss at step  16000 :  9.856259547114373
Average loss at step  18000 :  8.463812940776348
Average loss at step  20000 :  7.94557587480545
Nearest to have: be, are, were, eighth, algerian, mixer, was, adder,
Nearest to UNK: operatorname, agouti, dasyprocta, circ, vs, two, and, ulyanov,
Nearest to also: arica, responsible, luisa, novels, path, operatorname, dasyprocta, it,
Nearest to while: on, vedic, is, blended, manaus, valuation, december, triangular,
Nearest to only: operatorname, trapezohedron, technical, circ, humble, ep, and, wind,
Nearest to had: was, efforts, has, and, is, pregnancies, are, testing,
Nearest to eight: nine, five, zero, six, four, three, seven, agouti,
Nearest to its: the, jupiter, many, simultaneously, dasyprocta, his, their, circ,
Nearest to would: favors, rsha, thread, can, profitable, category, dasyprocta, reprinted,
Nearest to nine: eight, six, seven, zero, five, agouti, four, three,
Nearest to b: d, and, extends, elephant, lnot, aura, rebels, extremophiles,
Nearest to american: and, judge, tolstoy, deadweight, operatorname, jane, gestation, supposition,
Nearest to all: found, agouti, dasyprocta, defining, wheat, invaders, pogroms, wives,
Nearest to many: the, lager, fabian, proceeding, its, readers, vs, ungodly,
Nearest to during: in, transferred, observe, severed, and, caricatured, on, supporter,
Nearest to from: in, of, agouti, for, and, by, at, under,
Average loss at step  22000 :  6.9490511629581455
Average loss at step  24000 :  6.88644983792305
Average loss at step  26000 :  6.701062269926071
Average loss at step  28000 :  6.342500551581383
Average loss at step  30000 :  5.928834332227707
Nearest to have: are, be, were, had, has, eighth, astoria, abodes,
Nearest to UNK: operatorname, abet, agouti, four, dasyprocta, circ, six, shirkuh,
Nearest to also: it, arica, luisa, responsible, there, novels, dasyprocta, who,
Nearest to while: on, vedic, manaus, industrious, triangular, blended, though, valuation,
Nearest to only: operatorname, trapezohedron, humble, circ, technical, donnelly, ep, wind,
Nearest to had: was, has, have, were, efforts, and, is, are,
Nearest to eight: nine, six, seven, four, five, zero, three, agouti,
Nearest to its: the, their, many, his, dasyprocta, jupiter, arin, operatorname,
Nearest to would: can, rsha, favors, thread, salvage, but, profitable, dasyprocta,
Nearest to nine: eight, seven, six, five, four, zero, three, agouti,
Nearest to b: d, elephant, unnamed, extends, rebels, and, extremophiles, abet,
Nearest to american: and, judge, eucalyptus, supposition, deadweight, jane, tolstoy, abet,
Nearest to all: agouti, dasyprocta, found, defining, invaders, wives, wheat, ramirez,
Nearest to many: its, lager, the, fabian, proceeding, essendon, vs, readers,
Nearest to during: in, and, transferred, abbe, observe, severed, caricatured, siblings,
Nearest to from: in, by, agouti, and, at, of, for, with,
Average loss at step  32000 :  5.956870110034942
Average loss at step  34000 :  5.701302553534508
Average loss at step  36000 :  5.766676121473313
Average loss at step  38000 :  5.5207181915044785
Average loss at step  40000 :  5.264030146718025
Nearest to have: were, had, be, are, has, catania, astoria, abodes,
Nearest to UNK: abet, operatorname, agouti, dasyprocta, circ, four, truetype, three,
Nearest to also: which, it, there, luisa, novels, dasyprocta, who, arica,
Nearest to while: and, though, on, is, vedic, industrious, triangular, manaus,
Nearest to only: operatorname, technical, donnelly, trapezohedron, circ, ep, akita, humble,
Nearest to had: has, was, have, were, efforts, is, are, methionine,
Nearest to eight: six, nine, seven, four, three, five, zero, agouti,
Nearest to its: the, their, his, many, dasyprocta, some, operatorname, a,
Nearest to would: can, rsha, favors, may, thread, should, salvage, but,
Nearest to nine: eight, seven, six, five, four, zero, three, agouti,
Nearest to b: d, UNK, unnamed, elephant, zero, imran, abet, extends,
Nearest to american: and, eucalyptus, abet, jane, judge, seawater, supposition, deadweight,
Nearest to all: agouti, aveiro, dasyprocta, found, defining, wives, invaders, wheat,
Nearest to many: some, its, lager, fabian, frs, essendon, the, sargon,
Nearest to during: in, vma, and, on, after, transferred, abbe, from,
Nearest to from: in, agouti, by, of, under, with, at, on,
Average loss at step  42000 :  5.351674790143966
Average loss at step  44000 :  5.250400963425636
Average loss at step  46000 :  5.244674281835556
Average loss at step  48000 :  5.231675305485726
Average loss at step  50000 :  4.987441954255104
Nearest to have: had, are, be, were, has, nine, catania, abodes,
Nearest to UNK: abet, operatorname, agouti, dasyprocta, circ, four, truetype, three,
Nearest to also: which, it, who, there, still, often, luisa, arica,
Nearest to while: and, is, though, when, four, industrious, on, manaus,
Nearest to only: operatorname, ep, akita, circ, cnr, clockmaker, zorn, repeating,
Nearest to had: has, was, have, were, efforts, meredith, are, is,
Nearest to eight: six, nine, seven, four, five, three, agouti, zero,
Nearest to its: their, the, his, dasyprocta, some, many, operatorname, jupiter,
Nearest to would: can, rsha, may, will, favors, should, might, receives,
Nearest to nine: eight, six, seven, five, zero, three, agouti, four,
Nearest to b: d, franchisee, abet, UNK, imran, six, twh, operatorname,
Nearest to american: eucalyptus, abet, marcion, operatorname, jane, and, deadweight, community,
Nearest to all: agouti, two, dasyprocta, aveiro, three, found, both, wives,
Nearest to many: some, several, these, frs, its, essendon, lager, fabian,
Nearest to during: in, vma, and, after, abbe, severed, transferred, though,
Nearest to from: in, agouti, of, at, seven, under, with, for,
Average loss at step  52000 :  5.035931741476059
Average loss at step  54000 :  5.18924716758728
Average loss at step  56000 :  5.0329916459321975
Average loss at step  58000 :  5.060989990234375
Average loss at step  60000 :  4.925863807201385
Nearest to have: had, has, are, were, be, joram, abodes, okeh,
Nearest to UNK: abet, operatorname, michelob, agouti, ursus, dasyprocta, four, truetype,
Nearest to also: which, it, who, still, often, there, operatorname, dasyprocta,
Nearest to while: though, when, and, is, vedic, industrious, decimated, plexus,
Nearest to only: operatorname, clockmaker, cnr, akita, zorn, michelob, ursus, circ,
Nearest to had: has, have, was, were, efforts, meredith, lading, vdc,
Nearest to eight: six, nine, seven, four, five, three, zero, agouti,
Nearest to its: their, the, his, some, dasyprocta, many, operatorname, her,
Nearest to would: can, may, will, rsha, should, might, could, to,
Nearest to nine: eight, six, seven, five, four, zero, ursus, three,
Nearest to b: d, UNK, referral, franchisee, pulau, preceded, unnamed, imran,
Nearest to american: and, german, eucalyptus, marcion, jane, abet, british, operatorname,
Nearest to all: two, three, agouti, dasyprocta, aveiro, wives, some, both,
Nearest to many: some, several, these, essendon, frs, lager, its, other,
Nearest to during: in, after, vma, and, from, though, michelob, when,
Nearest to from: in, agouti, under, at, eight, by, between, of,
Average loss at step  62000 :  5.00042178106308
Average loss at step  64000 :  4.851484272003174
Average loss at step  66000 :  4.61413001549244
Average loss at step  68000 :  4.988944339632988
Average loss at step  70000 :  4.903946772933006
Nearest to have: had, has, were, are, be, joram, abodes, catania,
Nearest to UNK: operatorname, agouti, abet, michelob, ursus, upanija, thaler, mitral,
Nearest to also: which, often, it, still, who, now, microcebus, operatorname,
Nearest to while: thaler, though, when, and, is, although, vedic, decimated,
Nearest to only: operatorname, clockmaker, cnr, akita, michelob, ursus, zorn, circ,
Nearest to had: has, have, was, were, meredith, having, is, efforts,
Nearest to eight: six, seven, nine, five, four, three, zero, agouti,
Nearest to its: their, the, his, operatorname, her, abakan, some, dasyprocta,
Nearest to would: can, may, will, might, should, rsha, could, must,
Nearest to nine: eight, seven, six, five, four, zero, three, ursus,
Nearest to b: d, UNK, seven, franchisee, referral, pulau, six, abet,
Nearest to american: german, community, and, british, marcion, gestation, rhea, jane,
Nearest to all: thaler, some, agouti, dasyprocta, two, both, many, aveiro,
Nearest to many: some, several, these, other, frs, essendon, lager, all,
Nearest to during: in, after, microcebus, thaler, vma, when, from, siblings,
Nearest to from: in, agouti, under, into, at, with, thaler, between,
Average loss at step  72000 :  4.761852572202683
Average loss at step  74000 :  4.807222983121872
Average loss at step  76000 :  4.718639183044433
Average loss at step  78000 :  4.800889348119497
Average loss at step  80000 :  4.801087864398957
Nearest to have: had, has, were, are, be, marketable, joram, include,
Nearest to UNK: operatorname, agouti, abet, michelob, ursus, upanija, dasyprocta, six,
Nearest to also: which, often, still, it, now, iit, microcebus, operatorname,
Nearest to while: though, thaler, when, and, however, is, although, decimated,
Nearest to only: operatorname, clockmaker, cnr, akita, zorn, michelob, ursus, bckgr,
Nearest to had: has, have, was, were, meredith, having, been, lading,
Nearest to eight: nine, six, seven, five, four, zero, three, agouti,
Nearest to its: their, his, the, her, operatorname, abakan, some, dasyprocta,
Nearest to would: can, may, will, might, should, could, rsha, must,
Nearest to nine: eight, six, seven, five, four, zero, three, agouti,
Nearest to b: d, UNK, franchisee, pulau, abet, seven, imran, preceded,
Nearest to american: british, german, french, community, russian, abet, aba, rhea,
Nearest to all: some, thaler, many, agouti, both, two, dasyprocta, these,
Nearest to many: some, several, these, other, all, both, frs, essendon,
Nearest to during: in, after, microcebus, when, thaler, from, though, vma,
Nearest to from: in, under, agouti, at, into, during, of, by,
Average loss at step  82000 :  4.767960187554359
Average loss at step  84000 :  4.762628918051719
Average loss at step  86000 :  4.773108193635941
Average loss at step  88000 :  4.741913338899613
Average loss at step  90000 :  4.734284275054931
Nearest to have: had, has, were, are, be, having, include, joram,
Nearest to UNK: operatorname, michelob, abet, agouti, ursus, upanija, microcebus, dasyprocta,
Nearest to also: which, often, still, now, carnatic, it, sometimes, microcebus,
Nearest to while: and, though, thaler, when, however, although, but, operatorname,
Nearest to only: operatorname, clockmaker, cnr, zorn, akita, michelob, parentheses, bckgr,
Nearest to had: has, have, was, were, having, meredith, been, lading,
Nearest to eight: seven, nine, six, five, four, three, zero, agouti,
Nearest to its: their, his, the, her, some, abakan, operatorname, plutonium,
Nearest to would: can, may, will, might, should, could, rsha, must,
Nearest to nine: eight, seven, six, five, four, zero, three, agouti,
Nearest to b: d, UNK, franchisee, pulau, referral, four, imran, preceded,
Nearest to american: british, french, german, community, russian, supposition, aba, gestation,
Nearest to all: some, both, thaler, agouti, many, globemaster, these, dasyprocta,
Nearest to many: some, several, these, both, other, all, dasyprocta, frs,
Nearest to during: in, after, when, microcebus, thaler, under, and, though,
Nearest to from: in, agouti, under, at, into, with, thaler, on,
Average loss at step  92000 :  4.672305716276169
Average loss at step  94000 :  4.731088534712791
Average loss at step  96000 :  4.669641123414039
Average loss at step  98000 :  4.610346876502037
Average loss at step  100000 :  4.697692969560623
Nearest to have: had, has, were, are, be, moc, having, include,
Nearest to UNK: upanija, abet, operatorname, michelob, agouti, ursus, dasyprocta, stenella,
Nearest to also: which, often, still, now, it, sometimes, carnatic, who,
Nearest to while: though, when, thaler, although, however, but, and, is,
Nearest to only: operatorname, cnr, clockmaker, zorn, michelob, parentheses, akita, bckgr,
Nearest to had: has, have, was, were, having, meredith, is, moc,
Nearest to eight: seven, nine, six, five, four, zero, three, agouti,
Nearest to its: their, his, the, her, abakan, some, operatorname, plutonium,
Nearest to would: can, will, may, could, might, should, rsha, must,
Nearest to nine: eight, seven, six, zero, five, four, three, agouti,
Nearest to b: d, pulau, franchisee, twh, preceded, imran, abet, elephant,
Nearest to american: british, french, german, and, supposition, russian, specie, rhea,
Nearest to all: some, both, many, thaler, agouti, two, globemaster, these,
Nearest to many: some, several, these, all, both, other, most, dasyprocta,
Nearest to during: in, after, when, microcebus, thaler, under, though, stenella,
Nearest to from: in, agouti, under, into, through, with, at, by,

In [ ]:


